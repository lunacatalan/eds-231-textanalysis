---
title: "Lab5"
author: "Luna Herschenfeld-Catal√°n"
date: "2024-05-08"
output: html_document
---

### Lab 5 Assignment

#### Train Your Own Embeddings

```{r packages, include = FALSE}
library(tidytext)
library(tidyverse)
library(widyr) 
library(irlba)
library(broom) 
library(textdata)
library(ggplot2)
library(dplyr)
```

1.  Using the data from your Nexis Uni query from Week 2, create a set of word embeddings. To do this, you'll essentially need to recreate the steps in today's example down through the chunk named "pmi". 

```{r include = FALSE}
dir <- here::here("class", "data", "News_gw_policy")
#news_tbl <- read_csv(here::here("class", "data", "News_gw_policy", "news.csv"))
news_files <- list.files(pattern = ".docx", 
                         path = dir,
                         full.names = TRUE, recursive = TRUE, ignore.case = TRUE)
news_files

# take files list and read them into LNT object
dat <- LexisNexisTools::lnt_read(news_files, convert_date = FALSE, remove_cover = FALSE) 

# reference 'slot' using @
meta_df <- dat@meta
articles_df <- dat@articles
paragraphs_df <- dat@paragraphs


# create tibble with these columns
dat2 <- dplyr::tibble(Date = meta_df$Date,
               Headline = meta_df$Headline,
               id = articles_df$ID,
               text = articles_df$Article)

text_clean <- dat2 %>% 
  
  # remove the urls from the text
  separate_wider_delim(text, 
                       delim = "Delivered by Newstex)   ",
                       names = c("delete", "text"),
                       too_few = "align_end") %>% 
  select(-delete) %>% 
  
  # remove the notes at the end
  separate_wider_delim(text, 
                       delim = "The views expressed in any and all content distributed by Newstex",
                       names = c("text", "delete"),
                       too_few = "align_start") %>% 
  select(-delete) %>% 
  
  # remove any of the urls that start with https
  mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", "",
                     text)) 

```


```{r}

unigram_probs <- text_clean %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words, by = "word") %>% 
  count(word, sort = T) %>%  # counting frequencies of word appearnace
  mutate(p = n/sum(n))

skipgrams <- text_clean %>% 
  unnest_tokens(ngram, # creating an ngram object
                text, # using text
                token = "ngrams", # specifying the token
                n = 5) %>% # 5 word window
  mutate(ngramID = row_number()) %>% # create row ID
  tidyr::unite(skipgramID, # create new column
               id, ngramID) %>% # create unique ID with these rows
  unnest_tokens(word, ngram) %>% 
  anti_join(stop_words, by = "word")

skipgram_probs  <- skipgrams %>% 
  # within each 5 word ID, count each pair of words 
  widyr::pairwise_count(item = word,
                 feature = skipgramID,
                 upper = F) %>% 
  mutate(p = n/sum(n))

normalized_probs <- skipgram_probs %>% 
  rename(word1 = item1, 
         word2 = item2) %>% 
  left_join(unigram_probs %>% 
              select(word1 = word,
                     p1 = p),
            by = "word1") %>% 
  left_join(unigram_probs %>% 
              select(word2 = word,
                     p2 = p),
            by = join_by(word2)) %>% 
  mutate(p_together = p/p1/p2)

pmi_matrix <- normalized_probs %>% 
  mutate(pmi = log10(p_together)) %>% 
  cast_sparse(word1, word2, pmi)

dim(pmi_matrix)
```


2.  Think of 3 important words in your data set. Calculate and plot the 10 most semantically similar words for each of them. Identify and interpret any interesting or surprising results.

`protection`
`management`
`agricultural`

```{r}
search_synonym <- function(word_vectors, selected_vector, original_word) {
  
  data <- word_vectors %*% selected_vector
  similarities <- as.data.frame(data) %>% 
    tibble(token = rownames(data), # rownames are word labels
           similarity = data[,1]) %>% 
    filter(token != original_word) %>% # remove similarity with same word
    arrange(desc(similarity)) %>% 
    select(token, similarity)
  
  return(similarities)
}
```

Create the word vectors

```{r}

# replace any na values with 0
pmi_matrix@x[is.na(pmi_matrix@x)] <- 0

# perform decomposition
pmi_svd <- irlba::irlba(pmi_matrix, # matrix we want to factor
                        100, # the dimensions we want to reduce to
                        verbose = F)

word_vectors <- pmi_svd$u
rownames(word_vectors) <- rownames(pmi_matrix)

```

Find the synonyms for 3 words:
```{r}

protection <- search_synonym(word_vectors = word_vectors,
                       selected_vector = word_vectors["protection", ], # select specific word vector
                       original_word = "protection")


management <- search_synonym(word_vectors = word_vectors,
                       selected_vector = word_vectors["management", ], # select specific word vector
                       original_word = "management")

agricultural <- search_synonym(word_vectors = word_vectors,
                       selected_vector = word_vectors["agricultural", ], # select specific word vector
                       original_word = "agricultural")

```

Plot the similarities 
```{r}
protection %>% 
  mutate(selected = "protection") %>% 
  bind_rows(management %>% 
              mutate(selected = "management")) %>% 
  bind_rows(agricultural %>% 
              mutate(selected = "agricultural")) %>% 
  group_by(selected) %>% 
  top_n(10, similarity) %>% 
  mutate(token = reorder(token, similarity)) %>% 
  ggplot(aes(x = token, y = similarity,
             fill = selected)) +
  geom_col(show.legend = F) +
  facet_wrap(~selected, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = NULL, title = "Which word vectors are most similar to protection, management, or agricultural?")
```
Its interesting that in the top 10 words for protection that the word "lacking" shows up. This suggests that the context within these documents is highly associated with a lack of protection. Given the context of groundwater policy and knowing that there are declines in groundwater worldwide, it makes sense. The words most similar to management seem to be within the context of how things are managed (eg. plans, tools, governance) and what needs managed (eg. fisheries, drought, coastal). The most literal plot is the one for agricultural, with the most similar words being ones related to countries and economies. 

3.  Assemble 3 word math equations that you think could be useful or interesting for exploring the meaning of key words or ideas in your data set.

```{r}
groundwater_management <- word_vectors["groundwater",] + word_vectors["management",]

search_synonym(word_vectors = word_vectors,
                       selected_vector = groundwater_management, # select specific word vector
                       original_word = "")

groundwater_community<-  word_vectors["groundwater",] + word_vectors["community",]

search_synonym(word_vectors = word_vectors,
                       selected_vector = groundwater_community, # select specific word vector
                       original_word = "")

groundwater_no_community<-  word_vectors["groundwater",] - word_vectors["community",]

search_synonym(word_vectors = word_vectors,
                       selected_vector = groundwater_no_community, # select specific word vector
                       original_word = "")
```


#### Pretrained Embeddings

4.  Following the example in the SMLTR text (section 5.4), create a set of 100-dimensional GloVe word embeddings. These embeddings were trained by researchers at Stanford on 6 billion tokens from Wikipedia entries.

```{r}
options(timeout = 100)
glove6b <- read_delim(here::here("class/data/glove.6B.100d.txt"))
                    
```



5.  Test them out with the canonical word math equation on the GloVe embeddings: "berlin" - "germany" + "france" = ?

Hint: you'll need to convert the GloVe dataframe to a matrix and set the row names in order to use our synonym function.

6.  Recreate parts 2 and 3 above using the the GloVe embeddings in place of the ones you made. How do they compare? What are the implications for applications of these embeddings?
