---
title: "Lab 4"
author: "Your Name"
date: "2024-04-24"
output: html_document
---

Lab 4 Assignment: Due May 7 at 11:59pm

```{r packages, include = FALSE}
library(tidytext)
library(tidyverse)
library(tidymodels)
library(textrecipes)
library(discrim) # naive-bayes
```

1. Select another classification algorithm. 

I am going to do a random forest for the classification algorithm. 

Preprocessing: 
```{r message = FALSE, warning = FALSE}
# load in data
urlfile ="https://raw.githubusercontent.com/MaRo406/EDS-231-text-sentiment/main/data/climbing_reports_model_dat.csv"
incidents_df<-readr::read_csv(url(urlfile))

# split data
set.seed(1234)
incidents2class <- incidents_df %>% 
  mutate(fatal = factor(ifelse(is.na(Deadly),
                               "non-fatal",
                               "fatal")))

# look at the distribution of fatal and non-fatal
table(incidents2class$fatal)

incidents_split <- initial_split(incidents2class, 
                                 strata = fatal) # samples equally from each group

incidents_train <- training(incidents_split)
incidents_test <- testing(incidents_split)

# create recipe
recipe <- incidents_rec %>% 
  step_tokenize(Text) %>% 
  step_tokenfilter(Text, max_tokens = 1000) %>%  # only use 1000 words
  step_tfidf(Text)#coming from text recipes 
```


2. Conduct an initial out-of-the-box model fit on the training data and prediction on the test data.  Assess the performance of this initial model. 
```{r}
# create workflow
rf_wf <- workflow() %>% 
  add_recipe(recipe)
```

```{r}
# create specification - add mode and engine (random forest)
rf_spec <- rand_forest() %>% 
  set_mode("classification") %>% 
  set_engine("ranger")
```

```{r}
# fit the spec to the workflow
rf_fit <- rf_wf %>% 
  add_model(rf_spec) %>% 
  fit(data = incidents_train)

```



3. Select the relevant hyperparameters for your algorithm and tune your model.

4. Conduct a model fit using your newly tuned model specification.  How does it compare to your out-of-the-box model?

5.
  a. Use variable importance to determine the terms most highly associated with non-fatal reports?  What about terms associated with fatal reports? OR
  b. If you aren't able to get at variable importance with your selected algorithm, instead tell me how you might in theory be able to do it. Or how you might determine the important distinguishing words in some other way. 

6. Predict fatality of the reports in the test set.  Compare this prediction performance to that of the Naive Bayes and Lasso models.  Why do you think your model performed as it did, relative to the other two?
