---
title: "Lab5_Demo"
author: "Mateo Robbins"
date: "2024-05-08"
output: html_document
---

```{r packages, include = FALSE}
library(tidytext)
library(tidyverse)
library(widyr) 
library(irlba)
library(broom) 
library(textdata)
library(ggplot2)
library(dplyr)
```

#### Word Embeddings

We'll start off today by loading the climbing incident data again. This week we aren't interested in the fatality variable, just the text of the reports.

```{r data,}
incidents_df<-read_csv("https://raw.githubusercontent.com/MaRo406/EDS-231-text-sentiment/main/data/climbing_reports_model_dat.csv")
```

First, let's calculate the unigram probabilities -- how often we see each word in this corpus.

```{r unigrams}
unigram_probs <- incidents_df %>% 
  unnest_tokens(word, Text) %>% 
  anti_join(stop_words, by = "word") %>% 
  count(word, sort = T) %>%  # counting frequencies of word appearnace
  mutate(p = n/sum(n))

unigram_probs
```

OK, so that tells us the probability of each word.

Next, we need to know how often we find each word near each other word -- the skipgram probabilities. In this case we'll define the word context as a five-word window. We'll slide that window across all of our text and record which words occur together within that window.

We'll add an ngramID column that contains constituent information about each 5-gram we constructed by sliding our window.

```{r make-skipgrams}

skipgrams <- incidents_df %>% 
  unnest_tokens(ngram, # creating an ngram object
                Text, # using text
                token = "ngrams", # specifying the token
                n = 5) %>% # 5 word window
  mutate(ngramID = row_number()) %>% # create row ID
  tidyr::unite(skipgramID, # create new column
               ID, ngramID) %>% # create unique ID with these rows
  unnest_tokens(word, ngram) %>% 
  anti_join(stop_words, by = "word")
  
  
```

Now we use widyr::pairwise_count() to sum the total # of occurrences of each pair of words.

```{r pairwise_count}
skipgram_probs  <- skipgrams %>% 
  # within each 5 word ID, count each pair of words 
  widyr::pairwise_count(item = word,
                 feature = skipgramID,
                 upper = F) %>% 
  mutate(p = n/sum(n))

skipgram_probs 
```

The next step is to normalize these probabilities, that is, to calculate how often words occur together within a window, relative to their total occurrences in the data. We'll also harmnoize the naming conventions from the different functions we used.

```{r norm-prob}
normalized_probs <- skipgram_probs %>% 
  rename(word1 = item1, 
         word2 = item2) %>% 
  left_join(unigram_probs %>% 
              select(word1 = word,
                     p1 = p),
            by = "word1") %>% 
  left_join(unigram_probs %>% 
              select(word2 = word,
                     p2 = p),
            by = join_by(word2)) %>% 
  mutate(p_together = p/p1/p2)

# select the first 10 rows
normalized_probs[1:10,]
```

Now we have all the pieces to calculate the point-wise mutual information (PMI) measure. It's the logarithm of the normalized probability of finding two words together. PMI tells us which words occur together more often than expected based on how often they occurred on their own.

Then we cast to a matrix so we can use matrix factorization and reduce the dimensionality of the data.

```{r pmi}
pmi_matrix <- normalized_probs %>% 
  mutate(pmi = log10(p_together)) %>% 
  cast_sparse(word1, word2, pmi)

dim(pmi_matrix)
```

We do the singular value decomposition with irlba::irlba(). It's a "partial decomposition" as we are specifying a limited number of dimensions, in this case 100.

```{r svd}
# references non zero elements of matrix
# replace any na values with 0
pmi_matrix@x[is.na(pmi_matrix@x)] <- 0

# perform decomposition
pmi_svd <- irlba::irlba(pmi_matrix, # matrix we want to factor
                        100, # the dimensions we want to reduce to
                        verbose = F)

word_vectors <- pmi_svd$u
rownames(word_vectors) <- rownames(pmi_matrix)
```

These vectors in the "u" matrix are contain "left singular values". They are orthogonal vectors that create a 100-dimensional semantic space where we can locate each word. The distance between words in this space gives an estimate of their semantic similarity.

```{r syn_function}

search_synonym <- function(word_vectors, selected_vector, original_word) {
  
  data <- word_vectors %*% selected_vector
  similarities <- as.data.frame(data) %>% 
    tibble(token = rownames(data), # rownames are word labels
           similarity = data[,1]) %>% 
    filter(token != original_word) %>% # remove similarity with same word
    arrange(desc(similarity)) %>% 
    select(token, similarity)
  
  return(similarities)
}

```

Let's test it out!

```{r find-synonyms}

fall <- search_synonym(word_vectors = word_vectors,
                       selected_vector = word_vectors["fall", ], # select specific word vector
                       original_word = "fall")

fall

slip <- search_synonym(word_vectors = word_vectors,
                       selected_vector = word_vectors["slip", ], # select specific word vector
                       original_word = "slip")

slip

ice <- search_synonym(word_vectors = word_vectors,
                       selected_vector = word_vectors["ice", ], # select specific word vector
                       original_word = "ice")


snow <- search_synonym(word_vectors = word_vectors,
                       selected_vector = word_vectors["snow", ], # select specific word vector
                       original_word = "snow")

ice
snow
```

Here's a plot for visualizing the most similar words to a given target word.

```{r plot-synonyms}
slip %>% 
  mutate(selected = "slip") %>% 
  bind_rows(fall %>% 
              mutate(selected = "fall")) %>% 
  group_by(selected) %>% 
  top_n(15, similarity) %>% 
  mutate(token = reorder(token, similarity)) %>% 
  ggplot(aes(x = token, y = similarity,
             fill = selected)) +
  geom_col(show.legend = F) +
  facet_wrap(~selected, scales = "free") +
  coord_flip() +
  labs(x = NULL, title = "Which word vectors are most similar to slip or fall?")
```

One of the cool things about representing words as numerical vectors is that we can use math on those numbers that has some semantic meaning.

```{r word-math}

snow_danger <- word_vectors["snow",] + word_vectors["danger",]

search_synonym(word_vectors = word_vectors,
                       selected_vector = snow_danger, # select specific word vector
                       original_word = "")

no_snow_danger <-  word_vectors["danger",] - word_vectors["snow",]

search_synonym(word_vectors = word_vectors,
                       selected_vector = no_snow_danger, # select specific word vector
                       original_word = "")

```
