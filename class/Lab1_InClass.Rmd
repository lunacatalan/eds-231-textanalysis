---
title: "Lab 1: NYT API"
author: "Luna Catal√°n"
date: "2024-04-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite) #convert results from API queries into R-friendly formats 
library(tidyverse) #tidy
library(tidytext) #text data management and analysis
library(ggplot2) #plot word frequencies and publication dates

# for synonym
library(tm)

#assign API key.  When you create a NYT Dev account, you will be given a key
API_KEY <- "HpcxaZX9u9zu0dIGAGa6HcPAGOc4hzUY"
```

Today we will be grabbing some data from the New York Times database via their API, then running some basic string manipulations, trying out the tidytext format, and creating some basic plots.

<https://developer.nytimes.com/>

### Connect to the New York Times API and send a query

We have to decide which New York Times articles we are interested in examining. For this exercise, I chose articles about Deb Haaland, the current US Secretary of the Interior. As a member of the Laguna Pueblo Tribe, Haaland is the first Native American to serve as a Cabinet secretary. Very cool!

We'll send a query to the NY Times API using a URL that contains information about the articles we'd like to access.

fromJSON() is a wrapper function that handles our request and the API response. We'll use it to create an object, t, with the results of our query. The flatten = T argument converts from the nested JSON format to an R-friendlier form.

```{r eval = FALSE}

#create the query url
url <- paste("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=haaland&api-key=",API_KEY, sep ="")

#send the request, receive the response, and flatten
t <- fromJSON(url, flatten = T)
```

```{r api, eval = FALSE}
#what type of object is it?
class(t) # this is a list

#convert to a data frame 
t <- data.frame(t)

# how big is it?
dim(t)

# what fields are we working with?
colnames(t)

```

The name format, "response.xxx.xxx...", is a legacy of the JSON nested hierarchy.

Let's look at a piece of text. Our data object has a variable called "response.docs.snippet" that contains a short excerpt, or "snippet" from the article. Let's grab a snippet and try out some basic string manipulations from {stringr}.

```{r basic_stringr, eval=FALSE}
# look at snippet
t$response.docs.snippet[9]

#assign a snippet to x to use as fodder for stringr functions.  

x <- "Her nomination as secretary of the interior is historic, but as the first Native cabinet member, she would have to strike a delicate balance." 

#convert to lowercase.  
tolower(x)

#split into substrings
str_split(x, ",") # split on the commas

#swap strings - only the first occurace
str_replace(x, 'historic', 'without precedent')

#how do we replace all of them?
str_replace_all(x, " ", "_")

#detect a string - T/F
str_detect(x, "as")

#locate it
str_locate(x, "as") # first positions
str_locate_all(x, "as") # all positions

```

### OK, it's working but we want more data. Let's set some parameters for a bigger query.

```{r eval=FALSE}
term1 <- "Deb" 
term2 <- "&Haaland" # Need to use & to string  together separate terms
begin_date <- "20210120"
end_date <- "20230401"

#construct the query url using API operators
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term1,term2,"&begin_date=",begin_date,"&end_date=",end_date,"&facet_filter=true&api-key=",API_KEY, sep="")

#examine our query url
baseurl

```

The initial query returned one page of ten articles, but also gave us count of total number of hits on our query. We can use that to size a for() loop to automate requests.

```{r, eval=FALSE}
#run initial query
initialQuery <- fromJSON(baseurl)

# how many available hits in the query to know how many loops to do to get all of them
#maxPages <- round((initialQuery$response$meta$hits[1] / 10)-1) 
maxPages <- 10

#initiate a list to hold results of our for loop
pages_list <- list()

#loop
for(i in 1:maxPages){
  nytSearch <- fromJSON(paste0(baseurl, "&page=",i), flatten = TRUE) %>% 
    data.frame() # make call into a df
  message("Retrieving page ", i) # message for each item
  pages_list[[i+1]] <- nytSearch 
  Sys.sleep(16) # wait a certain amount of time
}
```

We converted each returned JSON object into a data frame.

```{r, bind_rows, eval=FALSE}

#bind the pages and create a tibble from nytDat
nyt_df <- bind_rows(pages_list)

names(nyt_df)
```

Let's start exploring our data.  What types of content did we turn up?
```{r article-type, eval=FALSE}
nyt_df %>% 
  group_by(response.docs.type_of_material) %>%
  summarize(count=n()) %>% #This creates a new data frame with the count of records for each type_of_material.
  mutate(percent = (count / sum(count))*100) %>% #add percent of total column
  ggplot() +
  geom_bar(aes(y=percent, x=response.docs.type_of_material, fill=response.docs.type_of_material), 
           stat = "identity") + 
  coord_flip()
```

```{r date-plot, eval=FALSE}

nyt_df %>%
  mutate(pubDay=gsub("T.*","",response.docs.pub_date)) %>% 
  group_by(pubDay) %>%
  summarise(count=n()) %>%
  filter(count >= 2) %>%
  ggplot() +
  geom_bar(aes(x=reorder(pubDay, count), y=count), stat="identity") +
  coord_flip() #bring date so bars go longwise
```

The New York Times doesn't make full text of the articles available through the API. But we can use the first paragraph of each article.

```{r plot_frequencies, eval=FALSE}
#find first paragraph field
names(nyt_df)
head(nyt_df[, 1:6])

# token is unit of analysis we have selected to evaluate our data - standard is a single work
tokenized <- nyt_df %>% 
  filter(response.docs.news_desk != "Sports") %>% # remove sports news
  filter(response.docs.news_desk != "Games") %>% 
  # take text, break it down to token level, each row corresponds to single token
  unnest_tokens(word, response.docs.lead_paragraph) # word is new column, paragraph is column we are unnesting from

#use tidytext::unnest_tokens to put in tidy form.  
tokenized[,"word"]
```

Alright, let's starting analyzing our data.  What the most frequent words in the articles we have?
```{r word_frequencies, eval=FALSE}
tokenized %>%
  count(word, sort = TRUE) %>% # sort by occurance of word
  filter(n > 50) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(x = n, y = word)) +
  geom_col() +
  labs(y = NULL)
```

Uh oh, who knows what we need to do here?

```{r stop-words, eval=FALSE}

#load stop words
data(stop_words)
stop_words

#stop word anti_join
tokenized <- tokenized %>% 
  anti_join(stop_words) # remove the stop_words

#now let's try that plot again
tokenized %>%
  count(word, sort = TRUE) %>% # sort by occurance of word
  filter(n > 15) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(x = n, y = word)) +
  geom_col() +
  labs(y = NULL)
```

OK, but look at the most common words. Does anything stand out?

```{r cleaning, eval=FALSE}

#inspect the list of tokens (words) 

# remove numbers
clean_tokens <-  str_remove_all(tokenized$word, "[:digit:]") # remove all numbers

## cleaning includes combining words that have 's
clean_tokens <- gsub("'s", "", clean_tokens)

tokenized$clean <- clean_tokens
# remove empty strings
tib <- subset(tokenized, clean != "")

tib %>%
  count(clean, sort = TRUE) %>%
  mutate(clean = reorder(clean, n)) %>%
  filter(n > 15) %>%
  ggplot(aes(n, clean)) +
  geom_col() +
  labs(y = NULL)


#try again

```

## Assignment (Due Tuesday 4/9 11:59pm)
Reminder: Please suppress all long and extraneous output from your submissions (ex:  lists of tokens).

1.  Create a free New York Times account (<https://developer.nytimes.com/get-started>)

2.  Pick an interesting environmental key word(s) and use the {jsonlite} package to query the API. Pick something high profile enough and over a large enough time frame that your query yields enough articles for an interesting examination.
 - Groundwater
 
```{r message=FALSE, warning = FALSE}

#create the query url
url <- paste("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=haaland&api-key=",API_KEY, sep ="")

#send the request, receive the response, and flatten
request <- fromJSON(url, flatten = T)

term1 <- "Groundwater" 
begin_date <- "20150101"
end_date <- "20240408"

#construct the query url using API operators
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",
                  term1, "%20",
                  "&begin_date=", begin_date,
                  "&end_date=", end_date,
                  "&facet_filter=true",
                  "&api-key=", API_KEY)

#run initial query
initialQuery <- fromJSON(baseurl)

#maxPages <- round((initialQuery$response$meta$hits[1] / 10)-1) 
maxPages <- 10

#initiate a list to hold results of our for loop
pages <- list()

#loop
for(i in 0:maxPages){
  nytSearch <- fromJSON(paste0(baseurl, "&page=",i), flatten = TRUE) %>% data.frame()
  message("Retrieving page ", i)
  pages[[i+1]] <- nytSearch 
  Sys.sleep(16)
}

#bind the pages and create a tibble from nytDat
nyt_df <- bind_rows(pages)
```


3.  Recreate the publications per day and word frequency plots using the first paragraph field.  This time filter on the response.docs.news_desk variable to winnow out irrelevant results.

-   Make some (at least 3) transformations to the corpus including: add context-specific stopword(s), stem a key term and its variants, remove numbers

Word frequency plot:
```{r message=FALSE, warning = FALSE}
nyt_df %>% 
  group_by(response.docs.type_of_material) %>%
  summarize(count=n()) %>% #This creates a new data frame with the count of records for each type_of_material.
  mutate(percent = (count / sum(count))*100) %>% #add percent of total column
  ggplot() +
  geom_bar(aes(y=percent, x=response.docs.type_of_material, fill=response.docs.type_of_material), 
           stat = "identity") + 
  coord_flip()

nyt_df %>% 
  group_by(response.docs.news_desk) %>%
  summarize(count=n()) %>% #This creates a new data frame with the count of records for each news outlet.
  mutate(percent = (count / sum(count))*100) %>% #add percent of total column
  ggplot() +
  geom_bar(aes(y=percent, x=response.docs.news_desk, fill=response.docs.news_desk), 
           stat = "identity") + 
  coord_flip()
```


```{r message=FALSE, warning = FALSE}
# token is unit of analysis we have selected to evaluate our data - standard is a single work
tokenized <- nyt_df %>% 
  filter(response.docs.news_desk != "Weekend") %>% # remove weekend news
  filter(response.docs.news_desk != "Travel") %>%
  filter(response.docs.news_desk != "Podcasts") %>%
  # take text, break it down to token level, each row corresponds to single token
  unnest_tokens(word, response.docs.lead_paragraph) # word is new column, paragraph is column we are unnesting from

# remove the stop words
tokenized <- tokenized %>% 
  anti_join(stop_words)

#tokenized$word

# remove numbers
token_clean <-  str_remove_all(tokenized$word, "[:digit:]")

## cleaning includes combining words that have 's
token_clean <- gsub("‚Äôs", "", token_clean)



# filter for the words
tokenized <- tokenized %>% 
  filter(word %in% token_clean) %>% # select the cleaned words 
  mutate(word = case_when(word == "calif" ~ "california",
                          word == "aquifers" ~ "aquifer",
                          TRUE ~ word)) %>% 
  filter(word != "york") %>% 
  filter(word != "time") %>% 
  filter(word != "times")
  
t_lead_plot <- tokenized %>%
  count(word, sort = TRUE) %>% # sort by occurance of word
  filter(n > 7) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(x = n, y = word)) +
  geom_col(fill = "darkblue") +
  labs(y = NULL,
       title = "Word Frequency in Lead Paragraph") +
  theme_bw()

t_lead_plot


```

Publications per day:
```{r message=FALSE, warning = FALSE}

tokenized %>%
  mutate(pubDay=gsub("T.*","",response.docs.pub_date)) %>% 
  group_by(pubDay) %>%
  summarise(count=n()) %>%
  filter(count >= 8) %>%
  ggplot() +
  geom_bar(aes(x=reorder(pubDay, count), y=count), stat="identity") +
  coord_flip() #bring date so bars go longwise

```

4.  Recreate the publications per day and word frequency plots using the headlines variable (response.docs.headline.main). Compare the distributions of word frequencies between the first paragraph and headlines. Do you see any difference?

```{r message=FALSE, warning = FALSE}
# token is unit of analysis we have selected to evaluate our data - standard is a single work
tokenized_headline <- nyt_df %>% 
  filter(response.docs.news_desk != "Weekend") %>% # remove weekend news
  filter(response.docs.news_desk != "Travel") %>%
  filter(response.docs.news_desk != "Podcasts") %>%
  # take text, break it down to token level, each row corresponds to single token
  unnest_tokens(word, response.docs.headline.main) # word is new column, paragraph is column we are unnesting from

# remove the stop words
tokenized_headline <- tokenized_headline%>% 
  anti_join(stop_words)

#tokenized$word

# remove numbers
token_clean <-  str_remove_all(tokenized_headline$word, "[:digit:]")

## cleaning includes combining words that have 's
token_clean <- gsub("‚Äôs", "", token_clean)



# filter for the words
tokenized_headline <- tokenized_headline %>% 
  filter(word %in% token_clean) %>% # select the cleaned words 
  mutate(word = case_when(word == "calif" ~ "california",
                          word == "aquifers" ~ "aquifer",
                          TRUE ~ word)) %>% 
  filter(word != "york") %>% 
  filter(word != "time") %>% 
  filter(word != "times")
  
t_head_plot <- tokenized_headline %>%
  count(word, sort = TRUE) %>% # sort by occurance of word
  filter(n > 7) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(x = n, y = word)) +
  geom_col(fill = "darkblue") +
  labs(y = NULL, 
       title = "Word Frequency in Main Headline") +
  theme_bw()

t_head_plot
```


Publications per day: 
```{r message=FALSE, warning = FALSE}

tokenized_headline %>%
  mutate(pubDay=gsub("T.*","",response.docs.pub_date)) %>% 
  group_by(pubDay) %>%
  summarise(count=n()) %>%
  filter(count >= 8) %>%
  ggplot() +
  geom_bar(aes(x=reorder(pubDay, count), y=count), stat="identity") +
  coord_flip() #bring date so bars go longwise

```

Comparing the word frequencies for the lead paragraph and the headlines:

The distribution of the words are very similar, since there are a lot of occurances of 1 word, and then it declines very rapidly from there. This follows zipfs law. In the lead paragraph there tends to be a greater magntitude of occurances, but the top 3 words are `water`, `groundwater`, and `california` in both the lead and main headline. This makes sense since the lead paragraph is longer, which means it has more words. A way to look at this would be to divide by the number of words total, which would give you a proportion. 

```{r message=FALSE, warning = FALSE}
ggpubr::ggarrange(t_lead_plot, t_head_plot)
```

