---
title: 'Lab 3 Demo: Topic Analysis'
author: "Mateo Robbins"
date: "2024-04-15"
output: html_document
---

```{r packages, warning=FALSE, message=FALSE}
library(quanteda)
library(tm)
library(topicmodels)
library(ldatuning)
library(tidyverse)
library(tidytext)
library(reshape2)
```

Load the data

```{r data, warning=FALSE, message=FALSE}
tbl <-read_csv("https://raw.githubusercontent.com/MaRo406/EDS-231-text-sentiment/main/data/tm_demo_dat.csv")
```

First we'll build the corpus using corpus() from {quanteda}.

```{r corpus}

corpus <- corpus(x = tbl, text_field = "text")

```

Next, we can use tokens(), also from {quanteda}, to construct a tokens object. tokens() takes a range of arguments related to cleaning the data. Next we'll create a stopwords lexicon and remove each word contained in it from our tokens object. The quanteda function tokens_select() lets us do the removal.

```{r tokens}
toks <- tokens(corpus, remove_punct = T,
               remove_numbers = T,
               remove_url = T)

# create stop word dictionary
add_stops <- stopwords(kind = quanteda_options(("language_stopwords")))

tok1 <- tokens_select(toks,
                     pattern = add_stops, # remove stop words
                     selection = "remove")
```

Now we can transform our data to prepare for topic modeling. Let's create a document-feature matrix with quanteda::dfm(). Topic modeling doesn't work with empty rows in this matrix, so we'll need to remove those. I do that here using {slam}, which is designed to deal with sparse matrices like ours.

```{r dfm}

dfm1 <- dfm(tok1,
            tolower = T) # make words lowercase

# remove the really infrequent words from document feature matrix
dfm2 <- dfm_trim(dfm1,
                 min_docfreq = 2) # must occur at least 2 times to stay

# if the sum across the row is greater than 0 we will keep it 
# in the case of having a document with only symbols when cleaning it the article row woul dbe empty and we would want to remove it
sel_ind <- slam::row_sums(dfm2) > 0    
dfm <- dfm2[sel_ind, ]
```
- in this case there were no empty documents 


Great, now we are almost ready to run a model. We just have to come up with an initial value for k, the number of latent topics present in the data. How do we do this? Let's say I think there may be political, economic and environmental articles. So I will tell the model to look for 3 topics by setting the k parameter = 3.

```{r LDA_modeling}

k = 3

topicModel_k3 <- LDA(dfm, # matrix
                     k, # number of topics
                     method = "Gibbs", # the upsampling process 
                     control = list(iter = 1000),
                     verbose = 25) # will update you every 25 iterations
```

Running topicmodels::LDA() produces an S3 object of class lda_topic_model which includes two posterior probability distributions: theta, a distribution over k topics within each document which givesð‘ƒ(topic|document)) and beta (in tidytext, but referred to as phi in other places), the distribution over v terms within each topic, where v is our vocabulary and gives ð‘ƒ(token|topic).

Let's examine at our results. posterior() extracts the theta and beta matrices.

```{r LDA_modeling}

results <- posterior(topicModel_k3)
attributes(results)

# distribution of terms on topics
beta <- results$terms

# distribution of topics across articles
theta <- results$topics

# shoul dhave 3 since k = 3
dim(beta)
dim(theta)

# look at the 10 most common words in the 3 topics
terms(topicModel_k3, 10)

```

Alright, so that worked out OK. An alternative to specifying k based on theory or a hypothesis is to run a series of models using a range of k values. ldatuning::FindTopicsNumber gives us the tools for this.

```{r find_k}

results <- FindTopicsNumber(dfm,
                            topics = seq(from = 2,
                                         to = 20,
                                         by = 1),
                            metrics = c("CaoJuan2009", "Deveaud2014"), # what to measure how good the number of topics 
                            method = "Gibbs",
                            verbose = T)

FindTopicsNumber_plot(results)
```

Alright, now let's estimate another model, this time with our new value of k.

```{r LDA_again}

# k could equal 7 or 12

k <- 7

topicModel_k7 <- LDA(dfm, # matrix
                     k, # number of topics
                     method = "Gibbs", # the upsampling process 
                     control = list(iter = 1000,
                                    verbose = 25))

tm7Results <- posterior(topicModel_k7)
terms(topicModel_k7, 10)

beta7 <- tm7Results$terms
# distribution of topics across articles
theta7 <- tm7Results$topics
vocab7 <- colnames(beta7)



k <- 12

topicModel_k12 <- LDA(dfm, # matrix
                     k, # number of topics
                     method = "Gibbs", # the upsampling process 
                     control = list(iter = 1000,
                                    verbose = 25))

tm12Results <- posterior(topicModel_k12)
terms(topicModel_k12, 10)

beta12 <- tm12Results$terms
# distribution of topics across articles
theta12 <- tm12Results$topics
vocab12 <- colnames(beta12)
```

There are multiple proposed methods for how to measure the best k value. You can go down the rabbit hole here: https://rpubs.com/siri/ldatuning

```{r top_terms_topic}

topics <- tidy(topicModel_k7, matrix = "beta")

top_terms <- topics %>% 
  group_by(topic) %>% 
  top_n(10, beta) %>% 
  ungroup() %>% 
  arrange(topic, -beta)

top_terms


```

```{r plot_top_terms}
top_terms %>% 
  mutate(term= reorder_within(term, beta, topic, sep = "")) %>% 
  ggplot(aes(x = term, y = beta, fill = factor(topic))) +
  geom_col(show.legend = F) +
  facet_wrap(~topic, scales = "free_y") +
  scale_x_reordered() +
  coord_flip()

```

Let's assign names to the topics so we know what we are working with.

```{r topic_names}

topic_words <- terms(topicModel_k7, 5) # top 5 words for each topic

# create topic names with first 5 words of the topics
topic_names <- apply(topic_words, 2,
                     paste, collapse = " ")

```

We can explore the theta matrix, which contains the distribution of each topic over each document.

```{r topic_dists}
#specify # of examples to inspect
example_ids <- c(1:5)
n <- length(example_ids)

# get topic proportions from example documents
example_props <- theta[example_ids,] # pull index ids
colnames(example_props) <- topic_names

#combine example topics with identifiers and melt to plotting form
viz_df <- melt(cbind(data.frame(example_props),
                     document = factor(1:n),
                     variable.names = "topic",
                     id.vars = "document"))

ggplot(data = viz_df,
       aes(x = variable, 
           y = value,
           fill = document),
       ylab = "proportion") +
  geom_bar(stat = "identity") +
  coord_flip() + 
  facet_wrap(~document, ncol = n)

```

Here's a neat JSON-based model visualizer, {LDAviz}. We can use this to visualize the words-on-topics distribution and intertopic distances. The size of the circles in the LDAvis plot show proportionally the amount of words that belong to each topic, and the space between circles shows the degree to which the circles share words.

```{r LDAvis}
library(LDAvis) #visualization 
library("tsne") #matrix decomposition
svd_tsne <- function(x) tsne(svd(x)$u)
json <- createJSON(
  phi = tm7Results$terms, 
  theta = tm7Results$topics, 
  doc.length = rowSums(dfm), 
  vocab = colnames(dfm), 
  term.frequency = colSums(dfm),
  mds.method = svd_tsne,
  plot.opts = list(xlab="", ylab="")
)
serVis(json)

```

The relevance parameter,Î»:  

Similar to tf-idf in its purpose.

When  Î» is close to 1, the relevance score emphasizes term frequency, making the interpretation focus on words that are common within the topic. 

When  Î» is lower, the score emphasizes the distinctiveness of terms, bringing out words that are unique to the topic even if they do not appear frequently

Relevance(w,t) = Î»Ã—P(wâˆ£t)+(1âˆ’Î»)Ã—P(wâˆ£t)/P(w) 

